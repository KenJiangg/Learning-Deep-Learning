{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchNNTutorial W/ Notes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KenJiangg/Learning-Deep-Learning/blob/master/PyTorchNNTutorial_W_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-fxsNOZDvBc3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Neural Networks from the PyTorch Tutorial\n",
        "  This is my take/interpretation of the PyTorch Tutorial and I attempt to explain some of the things I needed to learn as a complete beginner. "
      ]
    },
    {
      "metadata": {
        "id": "6oqyG0G21e0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Useful background knowledge about Neural Networks\n",
        "<a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\"> ReLu Definition </a> <br>\n",
        "<a href=\"https://en.wikipedia.org/wiki/Kernel_(image_processing)\"> Kernel Definition </a> <br>\n",
        "<a href = \"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\"> PyTorch Tutorial </a>  \n",
        "<a href=\"http://cs231n.stanford.edu/handouts/derivatives.pdf\"> Backpropagation </a>\n",
        "<br><br>\n",
        "**Brief Explanations about the concepts needed** <br>\n",
        "*ReLu* :<br>\n",
        "ReLu is a rectified linear unit. It is a type of activation function( Activation functions maps inputs that aren't neccessarily probabilistic to probabilistic values which are values either from -1 to 1 or 0 to 1.  Activation functions such as ReLu introduces non-linearity into neural networks because it's derivative is  $ f'(x) =\\begin{cases} 0,  & \\text{if $x$ < 0} \\\\ 1 & \\text{if $x$ > 0} \\end{cases}. $. \n",
        "ReLu is a popular activation function because it speeds up training; the gradient of a computation is either a 1 or 0 depending on whether x is negative or positive.\n",
        "<br><br>\n",
        "*Backpropagation*:<br>\n",
        "In a neural network, a single layer is typically a function of weights(*w*) and inputs(*x*). After processing a matrix through a neural network, it results in an output vector (*y*) and when we run a loss function on the vector; it results in a scalar loss (*L*). From this, assume we can compute $ \\frac{\\partial L}{\\partial y} $. The values we want to obtain are $ \\frac{\\partial L}{\\partial w} $ and $ \\frac{\\partial L}{\\partial x} $. One way of getting the two values is by computing $ \\frac{\\partial y}{\\partial w} $ and $ \\frac{\\partial y}{\\partial x} $ and use matrix multiplication to obtain the values. However, when working with typical neural networks, this method fails because of how much memory is needed to store the Jacobian matrix. For neural networks, we can actually avoid computing the Jacobian matrix by using small cases. <a href=\"http://cs231n.stanford.edu/handouts/derivatives.pdf\"> Math behind backpropagation and why we can use small cases </a>"
      ]
    },
    {
      "metadata": {
        "id": "S_VeOpGtbNZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning about Neural Networks \n",
        "\n",
        "  <img src=\"https://pytorch.org/tutorials/_images/mnist.png\"> \n",
        "  I had trouble understanding this picture and Neural Networks at first but what helped me understand the concept/design of a Feed-Forward Neural Network was matching the diagram to the code. <br><br>\n",
        "  INPUT -> C1(self.conv1 = nn.Conv2d(1,6,5)): <br>\n",
        "   Parameters for Conv2d in the tutorial are respecitvely input_channels(in this case it would be 1), output_channels(in this case it would be 6) and kernel size(in this case it is 5 which translates to a 5x5 kernel size). Review <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py\"> this link</a> for further information about the parameters for the convolutional layer. <br>\n",
        "  C2 -> S2 (x= F.max_pool2d(F.relu(self.conv1(x)), (2, 2))): <br>\n",
        "  From what I understand ReLu(rectified linear unit) and max_pool2d(pooling layer) is a subsampling method that PyTorch uses here to reduce the dimensionality of the feature map in order to increase efficiency.  <br> \n",
        "  S2 -> C3(self.conv2 = nn.Conv2d(6,16,5)): <br>\n",
        "  Same as INPUT->C1<br>\n",
        " C3 ->S4(x = F.max_pool2d(F.relu(self.conv2(x)), 2)): <br>\n",
        "  Same as C2->S2<br>\n",
        "  S4 -> C5(self.fc1 = nn.Linear(16 * 5 * 5, 120) &  x = F.relu(self.fc1(x))\n",
        " & x = x.view(-1, self.num_flat_features(x))): <br>\n",
        "  Here is where the matrixes/channels are flattened into one long vector. Specifically, it first flattens by the code(x = x.view(-1, self.num_flat_features(x))) where num_flat_features is a function defined in the class. Next, ReLu is applied to fc1(x) where fc1 is channel which it takes an input of 16 by 5 by 5(in step S2 -> C3, the output of it/ the input we are using now is 16 channels, in step C3-S4 -> the convolution layer is subsampled to a 5 x 5 layer) and outputs a vector of a size of 120. <br>\n",
        "  C5 -> F6(self.fc2 = nn.Linear(120, 84) & x = F.relu(self.fc2(x))):<br>\n",
        "  The input channel is defined as a size of 120 and output as a size of 84; ReLu is once again applied to this channel as a non-linear method. <br>\n",
        "  F6 -> OUTPUT(self.fc3 = nn.Linear(84, 10) & x = self.fc3(x)): <br>\n",
        "    Final step where input is a size of 84 and output is a size of 10. <br><br>\n",
        "  When running an image through these steps, we transform an image into a dense vector with a size of 10. Later on the tutorial, we learn how we can manipulate these vectors to reduce loss. "
      ]
    },
    {
      "metadata": {
        "id": "TsoOr01yuXHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "834fc208-5c75-4a65-860a-26346adf3dfa"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wfUIGcNwbaDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The learnable parameters of the neural net defined above is 10\n"
      ]
    },
    {
      "metadata": {
        "id": "c0vtcVD0ucyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a4f0395b-d4d5-47b0-9d67-2324bc3a2806"
      },
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6liYA8ddbo6m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we input a random number generated 32 by 32 matrix into the neural network, it outputs a vector with a size of 10.\n"
      ]
    },
    {
      "metadata": {
        "id": "uLFXbSSXuy68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c303c015-0ea5-4245-a73d-1ca037229cf1"
      },
      "cell_type": "code",
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0071,  0.0359, -0.0627, -0.0781, -0.0391, -0.1182, -0.0906, -0.1089,\n",
            "         -0.0271,  0.0129]], grad_fn=<ThAddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Go-c37Xau0Lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net.zero_grad() # zeroes the gradient buffer\n",
        "out.backward(torch.randn(1, 10)) # backpropagates with random gradients generated by torch.randn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MC3DfHU1brii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loss functions are a metric for finding out how far away the output is from the target. Here, we use net(input( a random 32x32 matrix)) as our output from the neural network and target(randn(10)) as the target variable we use to measure how far away our neural network outputs are from the actual values(in this case the actual values are randomly generated). The function above uses a Mean Squared Error as their type of loss function and we use this function to compare the output to target.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NK28k60vu1VW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36848a53-4074-4dbc-b5d8-7246752f4980"
      },
      "cell_type": "code",
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7968, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7DjOzxQWu3Pu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2ca549cb-dfcb-4854-ceb6-54a5d2441d57"
      },
      "cell_type": "code",
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f2613beca90>\n",
            "<ThAddmmBackward object at 0x7f2613becc88>\n",
            "<ExpandBackward object at 0x7f2613beca90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "upkc7pIXbzF9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After finding out the loss from above, we can actually apply this loss through ** backpropagation** and compute the gradients of each layer.\n"
      ]
    },
    {
      "metadata": {
        "id": "_-tmPyhCu5q3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ed84124f-1223-4757-a3fb-6b8e1130f821"
      },
      "cell_type": "code",
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([-0.0061,  0.0188, -0.0018, -0.0167, -0.0186,  0.0097])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQxWz5_9wk6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Update rules are needed to optimize neural networks because backpropagation computes the gradient but doesn't neccesarily do a good job in applying the gradient to the model."
      ]
    },
    {
      "metadata": {
        "id": "o5dHI-cDu7EP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giIS14Pg4oer",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the case of neural networks, you would likely want to use a premade update rule. In this case we use Stochastic Gradient Descent(SGD in the code).  Backpropagation computes to gradient for the model and SGD comes into to play by utilizing the gradients to update the weights of the nodes in the layers."
      ]
    },
    {
      "metadata": {
        "id": "pdY6g3nlu84_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ZOgBNfm5JgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0335637b-b96a-4254-f601-3f06a2b792e1"
      },
      "cell_type": "code",
      "source": [
        "print(net.conv1.bias.grad) #after using the stochastic gradient descent update rule"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.0049,  0.0070,  0.0058, -0.0181, -0.0141, -0.0063])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}